name: Build & test
on: [push]
env:
  BAZEL_ARGS: --repository_cache=~/repo-cache --disk_cache=~/disk-cache
  HADOOP_VERSION: 2.10.1
  SPARK_VERSION: 2.4.8
  SFL4J_VERSION: 1.7.30

jobs:
  build_and_test_with_linux:
    name: Run Build and Test with Linux Runner
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Install NixOS
        uses: cachix/install-nix-action@v18
        with:
          nix_path: nixpkgs=./nixpkgs.nix

      - name: Install cachix
        uses: cachix/cachix-action@v10
        with:
          name: tweag

      - name: Run cachix
        run: cachix watch-store tweag &

      - name: Configure
        run: mkdir -p ~/repo-cache ~/disk-cache

      - name: Prefetch Stackage snapshot
        run: nix-shell --pure --run "cmd='bazel fetch @stackage//... $BAZEL_ARGS'; \$cmd || \$cmd || \$cmd"

      - name: Build all
        run: |
          while true; do echo "."; sleep 60; done &
          nix-shell --pure --run "bazel build //apps/hello:sparkle-example-hello_deploy.jar $BAZEL_ARGS"
          nix-shell --pure --run "bazel build //apps/rdd-ops:sparkle-example-rddops_deploy.jar $BAZEL_ARGS"
          nix-shell --pure --run "bazel build //apps/osthreads:sparkle-example-osthreads_deploy.jar $BAZEL_ARGS"

      - name: Install Apache Spark and Hadoop
        run: |
          curl -OL https://repo1.maven.org/maven2/org/slf4j/slf4j-api/${SFL4J_VERSION}/slf4j-api-${SFL4J_VERSION}.jar
          curl -OL https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-without-hadoop.tgz
          tar -xzf spark-${SPARK_VERSION}-bin-without-hadoop.tgz
          curl -OL https://archive.apache.org/dist/hadoop/core/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz
          tar -xzf hadoop-${HADOOP_VERSION}.tar.gz

      - name: Run tests
        run: |
          export SPARK_DIST_CLASSPATH=$PWD/slf4j-api-${SFL4J_VERSION}.jar:$(hadoop-${HADOOP_VERSION}/bin/hadoop classpath)
          export PATH="$PWD/spark-${SPARK_VERSION}-bin-without-hadoop/bin:$PATH"
          spark-submit -v --executor-cores 1 --packages com.amazonaws:aws-java-sdk:1.11.920,org.apache.hadoop:hadoop-aws:2.8.4 bazel-bin/apps/hello/sparkle-example-hello_deploy.jar

  test:
    name: Run test
    runs-on: macos-11
    steps:

      - name: Checkout
        uses: actions/checkout@v2

      - name: Install NixOS
        uses: cachix/install-nix-action@v18
        with:
          nix_path: nixpkgs=./nixpkgs.nix

      - name: Install cachix
        uses: cachix/cachix-action@v10
        with:
          name: tweag

      - name: Run cachix
        run: cachix watch-store tweag &

      - name: Configure
        run: mkdir -p ~/repo-cache ~/disk-cache

      - name: Prefetch Stackage snapshot
        run: nix-shell --pure --run "cmd='bazel fetch @stackage//... $BAZEL_ARGS'; \$cmd || \$cmd || \$cmd"

      - name: Build all
        run: |
          while true; do echo "."; sleep 60; done &
          nix-shell --pure --run "bazel build //apps/hello:sparkle-example-hello_deploy.jar $BAZEL_ARGS"
          nix-shell --pure --run "bazel build //apps/rdd-ops:sparkle-example-rddops_deploy.jar $BAZEL_ARGS"
          nix-shell --pure --run "bazel build //apps/osthreads:sparkle-example-osthreads_deploy.jar $BAZEL_ARGS"

      - name: Install Apache Spark and Hadoop
        run: |
          curl -OL https://repo1.maven.org/maven2/org/slf4j/slf4j-api/${SFL4J_VERSION}/slf4j-api-${SFL4J_VERSION}.jar
          curl -OL https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-without-hadoop.tgz
          tar -xzf spark-${SPARK_VERSION}-bin-without-hadoop.tgz
          curl -OL https://archive.apache.org/dist/hadoop/core/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz
          tar -xzf hadoop-${HADOOP_VERSION}.tar.gz

      - name: Run tests
        run: |
          export SPARK_DIST_CLASSPATH=$PWD/slf4j-api-${SFL4J_VERSION}.jar:$(hadoop-${HADOOP_VERSION}/bin/hadoop classpath)
          export PATH="$PWD/spark-${SPARK_VERSION}-bin-without-hadoop/bin:$PATH"
          spark-submit -v --executor-cores 1 --packages com.amazonaws:aws-java-sdk:1.11.920,org.apache.hadoop:hadoop-aws:2.8.4 bazel-bin/apps/hello/sparkle-example-hello_deploy.jar
